# WIS Delft at HCOMP 2021 

We are thrilled to share that WIS Delft will be well-represented at AAAI HCOMP 2021, the premier international conference on Human Computation and Crowdsourcing. Research contributions from members of WIS Delft resulted in the acceptance of 3 full papers, 2 works-in-progress, 2 demonstrations, and 1 blue sky idea. Here’s a brief overview of the different contributions that will be presented at HCOMP 2021. 

## Full Research Papers

- [A Checklist to Combat Cognitive Biases in Crowdsourcing](https://ujwalgadiraju.com/Publications/HCOMP2021b.pdf): Tim Draws, Alisa Rieger, Oana Inel, Ujwal Gadiraju, Nava Tintarev

Recent research on data quality and the concomitant downstream effects on machine learning models and AI systems has demonstrated that cognitive biases can negatively affect the quality of crowdsourced data. Although this is now well-understood, cognitive biases often go unnoticed. Since significant efforts and costs entail the large-scale collection of human annotations across a plethora of tasks, there is unquestionable value in making such data collections reliable and resuable. To facilitate the reuse of crowdsourced data collections, practitioners can benefit from understanding whether and which cognitive biases may be associated with the data. To this end, task requesters (i.e., those who design and deploy tasks to gather labels or annotations online from a distributed crowd) need to ensure that the task workflows and design choices do not trigger cognitive biases of those who are contributing with their *human input*. Addressing this need in our work led by [Tim](https://timdraws.net), we propose the [**Cognitive Biases in Crowdsourcing Checklist** (CBC Checklist)](https://osf.io/g5b82/) as a practical tool that requesters can use to improve their task designs and appropriately describe potential limitations of collected data. We envsion this checklist to be a living document that can be extended by the community as and when new cognitive biases are discovered or understood to affect human input in crowdsourcing tasks. Collaborating on this work was inspiring for our team and we hope this can benefit the community! We invite you to read the paper to learn more about how the CBC Checklist can be used in practice. The paper also provides further analysis motivating the need for such a tool. 

- [Exploring the Music Perception Skills of Crowd Workers](https://ujwalgadiraju.com/Publications/HCOMP2021c.pdf): Ioannis Petros Samiotis, Sihang Qiu, Christoph Lofi, Jie Yang, Ujwal Gadiraju, Alessandro Bozzon

In this work led by [Petros](https://www.wis.ewi.tudelft.nl/samiotis), we explored the realm of music content annotation on crowdsourcing platforms. Annotating complex music artefacts dictates the need for certain skills and expertise. Traditional methods of participant selection are not designed to capture these kind of domain-specific skills and expertise. Despite the popularity of music annotation tasks and a need for such input at scale, we have a limited understanding of the distribution of musical properties among crowd workers – moreso in case of auditory perception skills. To address this knowledge gap, we conducted a user study (*N = 100*) on the Prolific crowdsourcing platform. We asked workers to indicate their musical sophistication through a questionnaire and assessed their music perception skills through an audio-based skill test. Our goal here was to establish a better understanding around the extent to which crowd workers possess auditory perceptions skills, beyond their own musical education level and self reported abilities. Our study shows that untrained crowd workers can possess high perception skills on the music elements of *melody*, *tuning*, *accent* and *tempo*; skills that can be useful in a wide range of annotation tasks in the music domain. Do read the paper to learn more about our work!

- [Making Time Fly: Using Fillers to Improve Perceived Latency in Crowd-Powered Conversational Systems](): 

## Works-in-Progress

- Exploring the Role of Domain Experts in Characterizing and Mitigating Machine Learning Errors

- The Role of Anthropomorphic Visual Cues in Human Interactions with Conversational Agents

## Demonstrations

- FindItOut: A Multiplayer GWAP for Collecting Plural Knowledge

- iClarify - A Tool to Help Requesters Iteratively Improve Task Descriptions in Crowdsourcing

## Blue Sky Idea

- The Science of Rejection: A Research Area for Human Computation

Do not hesitate to reach out to [us](u.k.gadiraju@tudelft.nl) in case you would like to hear more about our work, or for potential collaboration!
