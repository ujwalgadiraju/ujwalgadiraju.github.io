# WIS Delft at HCOMP 2021 

We are thrilled to share that WIS Delft will be well-represented at AAAI HCOMP 2021, the premier international conference on *Human Computation and Crowdsourcing*. Research contributions from members of WIS Delft resulted in the acceptance of 3 full papers, 2 works-in-progress, 2 demonstrations, and 1 blue sky idea. 

Here’s a brief overview of the different contributions that will be presented at HCOMP 2021. 

## Full Research Papers

- [A Checklist to Combat Cognitive Biases in Crowdsourcing](https://ujwalgadiraju.com/Publications/HCOMP2021b.pdf): Tim Draws, Alisa Rieger, Oana Inel, Ujwal Gadiraju, Nava Tintarev

Recent research on data quality and the concomitant downstream effects on machine learning models and AI systems has demonstrated that cognitive biases can negatively affect the quality of crowdsourced data. Although this is now well-understood, cognitive biases often go unnoticed. Since significant efforts and costs entail the large-scale collection of human annotations across a plethora of tasks, there is unquestionable value in making such data collections reliable and resuable. To facilitate the reuse of crowdsourced data collections, practitioners can benefit from understanding whether and which cognitive biases may be associated with the data. To this end, task requesters (i.e., those who design and deploy tasks to gather labels or annotations online from a distributed crowd) need to ensure that the task workflows and design choices do not trigger cognitive biases of those who are contributing with their *human input*. 

![](https://media.istockphoto.com/photos/head-with-a-puzzle-inside-and-an-inscription-bias-picture-id1321088893?b=1&k=20&m=1321088893&s=170667a&w=0&h=e-xe-v5ZC5gK92dCAQ4QQr1AWqVgeG9yFjgzSU-c8qY=)

Addressing this need in our work led by [Tim](https://timdraws.net), we propose the [**Cognitive Biases in Crowdsourcing Checklist** (CBC Checklist)](https://osf.io/g5b82/) as a practical tool that requesters can use to improve their task designs and appropriately describe potential limitations of collected data. We envsion this checklist to be a living document that can be extended by the community as and when new cognitive biases are discovered or understood to affect human input in crowdsourcing tasks. Collaborating on this work was inspiring for our team and we hope this can benefit the community! We invite you to read the paper to learn more about how the CBC Checklist can be used in practice. The paper also provides further analysis motivating the need for such a tool. 

- [Exploring the Music Perception Skills of Crowd Workers](https://ujwalgadiraju.com/Publications/HCOMP2021c.pdf): Ioannis Petros Samiotis, Sihang Qiu, Christoph Lofi, Jie Yang, Ujwal Gadiraju, Alessandro Bozzon

In this work led by [Petros](https://www.wis.ewi.tudelft.nl/samiotis), we explored the realm of music content annotation on crowdsourcing platforms. Annotating complex music artefacts dictates the need for certain skills and expertise. Traditional methods of participant selection are not designed to capture these kind of domain-specific skills and expertise. Despite the popularity of music annotation tasks and a need for such input at scale, we have a limited understanding of the distribution of musical properties among crowd workers – moreso in case of auditory perception skills. To address this knowledge gap, we conducted a user study (*N = 100*) on the Prolific crowdsourcing platform. We asked workers to indicate their musical sophistication through a questionnaire and assessed their music perception skills through an audio-based skill test. Our goal here was to establish a better understanding around the extent to which crowd workers possess auditory perceptions skills, beyond their own musical education level and self reported abilities. Our study shows that untrained crowd workers can possess high perception skills on the music elements of *melody*, *tuning*, *accent* and *tempo*; skills that can be useful in a wide range of annotation tasks in the music domain. Do read the paper to learn more about our work! A fun fact here is that Petros has received formal piano training in a conservatory for about 12 years, where he also studied music theory and harmony for nearly 6 years. Petros took violin lessons for around 1.5 years, but hit pause on that button to pursue a PhD!

- [Making Time Fly: Using Fillers to Improve Perceived Latency in Crowd-Powered Conversational Systems](https://ujwalgadiraju.com/Publications/HCOMP2021a.pdf): Tahir Abbas, Ujwal Gadiraju, Vassilis-Javed Khan, Panos Markopoulos

In this work led by [Tahir](), we explored the challenge of dealing with latency in crowd-powered conversational systems (CPCS). Such systems are gaining traction due to their potential utility in a range of application fields where automated conversational interfaces are still inadequate. Currently, long response times negatively impact CPCSs, limiting their potential application as conversational partners. Related research has focused on developing algorithms for swiftly hiring workers and synchronous crowd coordination techniques to ensure high-quality work. Evaluation studies typically concern system reaction times and performance measurements, but have so far not examined the effects of extended wait times on users. The goal of this study, based on time perception models, is to explore how effective different time fillers are at reducing the negative impacts of waiting in CPCSs. To this end, we conducted a rigorous simulation-based between-subjects (N = 930) study on the Prolific crowdsourcing platform to assess the influence of different filler types across three levels of delay (8, 16 & 32s) for Information Retrieval (IR) and stress management tasks. Our re- sults show that asking users to perform secondary tasks (e.g., microtasks or breathing exercises) while waiting for longer periods of time helped divert their atten- tion away from timekeeping, increased their engagement, and resulted in shorter perceived waiting times. For shorter delays, conversational fillers generated more intense immersion and contributed to shorten the perception of time.

## Works-in-Progress


- [The Role of Anthropomorphic Visual Cues in Human Interactions with Conversational Agents](https://ujwalgadiraju.com/Publications/HCOMP2021d.pdf): Emilija Zlatkute, Sihang Qiu, Jie Yang, Ujwal Gadiraju

- [Exploring the Role of Domain Experts in Characterizing and Mitigating Machine Learning Errors](https://yangjiera.github.io/pdf/hoogland2021hcomp.pdf): 


## Demonstrations

- [FindItOut: A Multiplayer GWAP for Collecting Plural Knowledge](https://ujwalgadiraju.com/Publications/HCOMP2021e.pdf): Agathe Balayn, Gaole He, Andrea Hu, Jie Yang, Ujwal Gadiraju

- [iClarify - A Tool to Help Requesters Iteratively Improve Task Descriptions in Crowdsourcing](https://ujwalgadiraju.com/Publications/HCOMP2021f.pdf):

## Blue Sky Idea

- [The Science of Rejection: A Research Area for Human Computation](https://yangjiera.github.io/pdf/sayin2021hcomp.pdf): Burcu Sayin, Jie Yang, Andrea Passerini, Fabio Casati

Do not hesitate to reach out to [us](u.k.gadiraju@tudelft.nl) in case you would like to hear more about our work, or for potential collaboration!
