# WIS Delft at HCOMP 2021 

We are excited to share that WIS Delft will be very well represented at AAAI HCOMP 2021, the premier international conference on Human Computation and Crowdsourcing. Research contributions from members of WIS Delft resulted in the acceptance of 3 full papers, 2 works-in-progress, 2 demonstrations, and 1 blue sky idea. Hereâ€™s a brief overview of the different contributions that will be presented at HCOMP 2021. 

## Full Research Papers

- [A Checklist to Combat Cognitive Biases in Crowdsourcing](https://ujwalgadiraju.com/Publications/HCOMP2021b.pdf): Tim Draws, Alisa Rieger, Oana Inel, Ujwal Gadiraju, Nava Tintarev

Recent research on data quality and the concomitant downstream effects on machine learning models and AI systems has demonstrated that cognitive biases can negatively affect the quality of crowdsourced data. Although this is now well-understood, cognitive biases often go unnoticed. Since a significant efforts and costs entail the large-scale collection of human annotations across a plethora of tasks, there is unquestionable value in making such data collections reliable and resuable. To facilitate the reuse of crowdsourced data collections, practitioners can benefit from understanding whether and which cognitive biases may be associated with the data. To this end, task requesters (i.e., those who design and deploy tasks to gather labels or annotations online from a distributed crowd) need to ensure that the task workflows and design choices do not trigger cognitive biases of those who are contributing with their *human input*. Addressing this need,we propose the [**Cognitive Biases in Crowdsourcing Checklist** (CBC Checklist)](https://osf.io/g5b82/) as a practical tool that requesters can use to improve their task designs and appropriately describe potential limitations of collected data. We envsion this checklist to be a living document that can be extended by the community as and when new cognitive biases are discovered or understood to affect human input in crowdsourcing tasks. We invite you to read the paper to learn more about how the CBC Checklist can be used in practice and further analysis that motivates the need for such a tool. 

- Exploring the Music Perception Skills of Crowd Workers
- Making Time Fly: Using Fillers to Improve Perceived Latency in Crowd-Powered Conversational Systems

## Works-in-Progress

- Exploring the Role of Domain Experts in Characterizing and Mitigating Machine Learning Errors

- The Role of Anthropomorphic Visual Cues in Human Interactions with Conversational Agents

## Demonstrations

- FindItOut: A Multiplayer GWAP for Collecting Plural Knowledge

- iClarify - A Tool to Help Requesters Iteratively Improve Task Descriptions in Crowdsourcing

## Blue Sky Idea

- The Science of Rejection: A Research Area for Human Computation

Do not hesitate to reach out to [us](u.k.gadiraju@tudelft.nl) in case you would like to hear more about our work, or for potential collaboration!
